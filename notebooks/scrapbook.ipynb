{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network to recognize Quick Draw doodles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea of this notebook will be to try and test the solutions provided by Google AI and other kagglers to the classification problem linked to the Quick Draw dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google AI suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Neural Network dataset introducting LSTM layers. Indeed, they want to take into account the notion of temporality by using layers that keep track of what happened before. They also do not want to use a Neural Network that would consider the input as an image by using 1D Convolutionnal Layers. It's worth a try ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial of Google AI's solution in Tensorflow right here : https://www.tensorflow.org/tutorials/sequences/recurrent_quickdraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considering the dataset as images only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simpler idea would consist in interpreting the vectors of coordinates of the dataset as images only. Thus, we could use classical Neural Networks architectures that would consist in a succession of 2D Convolutionnal Layers and Max Pooling Layers, ending with a Flatten Layer then a FC layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple of such a solution in Tensorflow here : https://www.kaggle.com/gaborfodor/black-white-cnn-lb-0-75/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 80\n",
    "test_size = 20\n",
    "total_size = train_size + test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '..\\\\data\\\\train_simplified' # use your path\n",
    "allFiles = glob.glob(path + \"\\\\*.csv\")\n",
    "frame = pd.DataFrame()\n",
    "trainlist_ = []\n",
    "testlist_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col='key_id',nrows=100).drop(['countrycode', 'recognized', 'timestamp'], axis=1)[:total_size]\n",
    "    trainlist_.append(df[:train_size])\n",
    "    testlist_.append(df[train_size:total_size])\n",
    "data_train = pd.concat(trainlist_)\n",
    "data_test = pd.concat(testlist_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drawing = data_train.drawing.map(ast.literal_eval)\n",
    "data_test.drawing = data_test.drawing.map(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = {'train':data_train, 'test':data_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, data,  base_size = 255, size = None, lw = 6):\n",
    "        'Initialization'\n",
    "        self.lw = lw\n",
    "        if size == None:\n",
    "            self.size = base_size\n",
    "        self.base_size = base_size\n",
    "        self.data = data\n",
    "        self.labels = data.word\n",
    "        self.drawing = data.drawing\n",
    "\n",
    "    def draw_cv2(self, raw_strokes):\n",
    "        img = np.zeros((self.base_size, self.base_size), np.uint8)\n",
    "        for stroke in raw_strokes:\n",
    "            for i in range(len(stroke[0]) - 1):\n",
    "                _ = cv2.line(img, (stroke[0][i], stroke[1][i]), (stroke[0][i + 1], stroke[1][i + 1]), 255, self.lw)\n",
    "        if self.size != self.base_size:\n",
    "            return cv2.resize(img, (self.size, self.size))\n",
    "        else:\n",
    "            return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.drawing)\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        key_id = self.data.index[ind]\n",
    "\n",
    "        # Load data and get label\n",
    "        raw_strokes = self.data.drawing[key_id]\n",
    "        X = torch.from_numpy(self.draw_cv2(raw_strokes))\n",
    "        y = self.labels[key_id]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Dataset(partition['train'])\n",
    "train_generator = torch.utils.data.DataLoader(train_set, batch_size=5, shuffle=True)\n",
    "\n",
    "test_set = Dataset(partition['test'])\n",
    "test_generator = torch.utils.data.DataLoader(test_set,  batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "for data in train_generator:\n",
    "    if count == 1:\n",
    "        inputs_try,labels_try = data\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([255, 255])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_try[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'anvil')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(inputs_try[0])\n",
    "plt.title(labels_try[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "?F.avg_pool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_model):\n",
    "        super(classifier, self).__init__()\n",
    "        self.conv1 = nn.conv2d(in_channels=1, our_channels=64, kernel_size=3, stride=2, padding=3)\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.base = nn.Sequential(*list(pretrained_model.children())[:-1])\n",
    "        self.linear = nn.Linear(in_features=512, out_features=340)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.base(x)\n",
    "        x = F.avg_pool2d(x, x.size()[2:])\n",
    "        f = x.view(x.size(0), -1)\n",
    "        y = self.linear(f)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = torchvision.models.resnet18(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_class = classifier(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [64, 3, 7, 7], but got input of size [5, 255, 255] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-c600eebbe062>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_3images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_try\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mconv_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_3images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-9cfa24c46fb1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 301\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [64, 3, 7, 7], but got input of size [5, 255, 255] instead"
     ]
    }
   ],
   "source": [
    "batch_3images = inputs_try.type(torch.FloatTensor)\n",
    "conv_class(batch_3images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
